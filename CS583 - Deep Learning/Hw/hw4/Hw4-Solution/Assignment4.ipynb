{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CERWeJIMPwGY"
      },
      "source": [
        "# Assignment 4: Build a Supervised Autoencoder.\n",
        "\n",
        "### Name: Harris Spahic\n",
        "\n",
        "### Due Date: Tuesday 5/2/2023 11:59PM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5qGf_LLPwGZ"
      },
      "source": [
        "\n",
        "PCA and the standard autoencoder are unsupervised dimensionality reduction methods, and their learned features are not discriminative. If you build a classifier upon the low-dimenional features extracted by PCA and autoencoder, you will find the classification accuracy very poor.\n",
        "\n",
        "Linear discriminant analysis (LDA) is a traditionally supervised dimensionality reduction method for learning low-dimensional features which are highly discriminative. Likewise, can we extend autoencoder to supervised leanring?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SIFKjnABPwGZ"
      },
      "source": [
        "**You are required to build and train a supervised autoencoder look like the following.** You are required to add other layers properly to alleviate overfitting.\n",
        "\n",
        "\n",
        "![Network Structure](https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM5/supervised_ae.png?raw=true \"NetworkStructure\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fNohSFbFPwGa"
      },
      "source": [
        "## 0. You will do the following:\n",
        "\n",
        "1. Build a standard dense autoencoder, visual the low-dim features and the reconstructions, and evaluate whether the learned low-dim features are discriminative.\n",
        "\n",
        "2. Repeat the above process by training a supervised autoencoder.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziHiYnJwPwGa"
      },
      "source": [
        "## 1. Data preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTcBJb-WPwGa"
      },
      "source": [
        "### 1.1. Load data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZ8T6_AxPwGa"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = x_train.reshape(60000, 28*28).astype('float32') / 255.\n",
        "x_test = x_test.reshape(10000, 28*28).astype('float32') / 255.\n",
        "\n",
        "print('Shape of x_train: ' + str(x_train.shape)) \n",
        "print('Shape of x_test: ' + str(x_test.shape))\n",
        "print('Shape of y_train: ' + str(y_train.shape))\n",
        "print('Shape of y_test: ' + str(y_test.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rLwXo0yPwGb"
      },
      "source": [
        "### 1.2. One-hot encode the labels\n",
        "\n",
        "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
        "\n",
        "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
        "\n",
        "2. Apply the function to ```y_train``` and ```y_test```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YyI16V84PwGb"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def to_one_hot(y, num_class=10):\n",
        "    results = np.zeros((len(y), num_class))\n",
        "    for i, label in enumerate(y):\n",
        "        results[i, label] = 1.\n",
        "    return results\n",
        "\n",
        "y_train_vec = to_one_hot(y_train)\n",
        "y_test_vec = to_one_hot(y_test)\n",
        "\n",
        "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
        "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
        "\n",
        "print(y_train[0])\n",
        "print(y_train_vec[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbfl7Um8PwGc"
      },
      "source": [
        "### 1.3. Randomly partition the training set to training and validation sets\n",
        "\n",
        "Randomly partition the 60K training samples to 2 sets:\n",
        "* a training set containing 10K samples;\n",
        "* a validation set containing 50K samples. (You can use only 10K to save time.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnjlaZzuPwGc"
      },
      "outputs": [],
      "source": [
        "rand_indices = np.random.permutation(60000)\n",
        "train_indices = rand_indices[0:10000]\n",
        "valid_indices = rand_indices[10000:20000]\n",
        "\n",
        "x_val = x_train[valid_indices, :]\n",
        "y_val = y_train_vec[valid_indices, :]\n",
        "\n",
        "x_tr = x_train[train_indices, :]\n",
        "y_tr = y_train_vec[train_indices, :]\n",
        "\n",
        "print('Shape of x_tr: ' + str(x_tr.shape))\n",
        "print('Shape of y_tr: ' + str(y_tr.shape))\n",
        "print('Shape of x_val: ' + str(x_val.shape))\n",
        "print('Shape of y_val: ' + str(y_val.shape))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cVjMusdlPwGc"
      },
      "source": [
        "## 2. Build an unsupervised  autoencoder and tune its hyper-parameters\n",
        "\n",
        "1. Build a dense autoencoder model\n",
        "2. Your encoder should contain 3 dense layers and 1 bottlenect layer with 2 as  output size. \n",
        "3. Your decoder should contain 4 dense layers with 784 as output size.\n",
        "4. You can choose different number of hidden units in dense layers.\n",
        "5. Do not add other layers (no activation layers), you may add them in later sections.\n",
        "6. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
        "    * Do NOT use test data for hyper-parameter tuning!!!\n",
        "    \n",
        "7. Try to achieve a validation loss as low as possible.\n",
        "8. Evaluate the model on the test set.\n",
        "9. Visualize the low-dim features and reconstructions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbKlAcMFPwGc"
      },
      "source": [
        "### 2.1. Build the model (20 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dL5MH6UlPwGc"
      },
      "outputs": [],
      "source": [
        "from keras.layers import *\n",
        "from keras import models\n",
        "\n",
        "input_img = Input(shape=(784,), name='input_img')\n",
        "\n",
        "encode1 = Dense(400, activation='relu')(input_img)\n",
        "encode2 = Dense(100, activation='relu')(encode1)\n",
        "encode3 = Dense(50, activation='relu')(encode2)\n",
        "\n",
        "bottleneck = Dense(2, activation='relu')(encode3)\n",
        "\n",
        "decode1 = Dense(50, activation='relu')(bottleneck)\n",
        "decode2 = Dense(100, activation='relu')(decode1)\n",
        "decode3 = Dense(400, activation='relu')(decode2)\n",
        "decode4 = Dense(784, activation='relu')(decode3)\n",
        "\n",
        "ae = models.Model(input_img, decode4)\n",
        "\n",
        "ae.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mq0XpDhTPwGd"
      },
      "outputs": [],
      "source": [
        "# print the network structure to a PDF file\n",
        "\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot, plot_model\n",
        "\n",
        "SVG(model_to_dot(ae, show_shapes=False).create(prog='dot', format='svg'))\n",
        "\n",
        "plot_model(\n",
        "    model=ae, show_shapes=False,\n",
        "    to_file='unsupervised_ae.pdf'\n",
        ")\n",
        "\n",
        "# you can find the file \"unsupervised_ae.pdf\" in the current directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SSf8i8QHPwGd"
      },
      "source": [
        "### 2.2. Train the model and tune the hyper-parameters (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viuWz8tWPwGd"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "learning_rate = 1E-3 # to be tuned!\n",
        "\n",
        "ae.compile(loss='mean_squared_error',\n",
        "           optimizer=optimizers.RMSprop(learning_rate=learning_rate))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0DeL_ZDzPwGd"
      },
      "outputs": [],
      "source": [
        "history = ae.fit(x_tr, x_tr, \n",
        "                 batch_size=128, \n",
        "                 epochs=100, \n",
        "                 validation_data=(x_val, x_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sTeRNc6ePwGe"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(loss))\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGCqJmkZPwGe"
      },
      "source": [
        "### 2.3. Visualize the reconstructed test images (5 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sk0TixTcPwGe"
      },
      "outputs": [],
      "source": [
        "ae_output = ae.predict(x_test).reshape((10000, 28, 28))\n",
        "\n",
        "ROW = 5\n",
        "COLUMN = 4\n",
        "\n",
        "x = ae_output\n",
        "fname = 'reconstruct_ae.pdf'\n",
        "\n",
        "fig, axes = plt.subplots(nrows=ROW, ncols=COLUMN, figsize=(8, 10))\n",
        "for ax, i in zip(axes.flat, np.arange(ROW*COLUMN)):\n",
        "    image = x[i].reshape(28, 28)\n",
        "    ax.imshow(image, cmap='gray')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(fname)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XC9oKRltPwGe"
      },
      "source": [
        "### 2.4. Evaluate the model on the test set\n",
        "\n",
        "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viaZ0vGXPwGe"
      },
      "outputs": [],
      "source": [
        "loss = ae.evaluate(x_test, x_test)\n",
        "print('loss = ' + str(loss))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiH0YYflPwGe"
      },
      "source": [
        "### 2.5. Visualize the low-dimensional features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjNJXDsIPwGf"
      },
      "outputs": [],
      "source": [
        "# build the encoder network\n",
        "ae_encoder = models.Model(input_img, bottleneck)\n",
        "ae_encoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KufyF8uPwGf"
      },
      "outputs": [],
      "source": [
        "# extract low-dimensional features from the test data\n",
        "encoded_test = ae_encoder.predict(x_test)\n",
        "print('Shape of encoded_test: ' + str(encoded_test.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RdA-flR5PwGf"
      },
      "outputs": [],
      "source": [
        "colors = np.array(['r', 'g', 'b', 'm', 'c', 'k', 'y', 'purple', 'darkred', 'navy'])\n",
        "colors_test = colors[y_test]\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "plt.scatter(encoded_test[:, 0], encoded_test[:, 1], s=10, c=colors_test, edgecolors=colors_test)\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "fname = 'ae_code.pdf'\n",
        "plt.savefig(fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr0UMMcAPwGf"
      },
      "source": [
        "#### Remark:\n",
        "\n",
        "Judging from the visualization, the low-dim features seems not discriminative, as 2D features from different classes are mixed. Let quantatively find out whether they are discriminative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjRVaPmvPwGf"
      },
      "source": [
        "## 3. Are the learned low-dim features discriminative? (10 points)\n",
        "\n",
        "To find the answer, lets train a classifier on the training set (the extracted 2-dim features) and evaluation on the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPvkWFAvPwGf"
      },
      "outputs": [],
      "source": [
        "# extract the 2D features from the training, validation, and test samples\n",
        "f_tr = ae_encoder.predict(x_tr)\n",
        "f_val = ae_encoder.predict(x_val)\n",
        "f_te = ae_encoder.predict(x_test)\n",
        "\n",
        "print('Shape of f_tr: ' + str(f_tr.shape))\n",
        "print('Shape of f_te: ' + str(f_te.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nsp4Gbr9PwGf"
      },
      "outputs": [],
      "source": [
        "from keras.layers import Dense, Input\n",
        "from keras import models\n",
        "\n",
        "input_feat = Input(shape=(2,))\n",
        "\n",
        "hidden1 = Dense(128, activation='relu')(input_feat)\n",
        "hidden2 = Dense(128, activation='relu')(hidden1)\n",
        "output = Dense(10, activation='softmax')(hidden2)\n",
        "\n",
        "classifier = models.Model(input_feat, output)\n",
        "\n",
        "classifier.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qd2igsB6PwGg"
      },
      "outputs": [],
      "source": [
        "classifier.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=optimizers.RMSprop(learning_rate=1E-4),\n",
        "                  metrics=['acc'])\n",
        "\n",
        "history = classifier.fit(f_tr, y_tr, \n",
        "                        batch_size=32, \n",
        "                        epochs=30, \n",
        "                        validation_data=(f_val, y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kz_ewffqPwGg"
      },
      "source": [
        "### Conclusion\n",
        "\n",
        "Using the 2D features, the validation accuracy is 60~70%. Recall that using the original data, the accuracy is about 97%. Obviously, the 2D features are not very discriminative.\n",
        "\n",
        "We are going to build a supervised autoencode model for learning low-dimensional discriminative features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2nW56WAPwGg"
      },
      "source": [
        "## 4. Build a supervised autoencoder model\n",
        "\n",
        "\n",
        "**You are required to build and train a supervised autoencoder look like the following.** (Not necessary the same. You can use convolutional layers as well.) You are required to add other layers properly to alleviate overfitting.\n",
        "\n",
        "\n",
        "![Network Structure](https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM5/supervised_ae.png?raw=true \"NetworkStructure\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ec-KwvH9PwGg"
      },
      "source": [
        "### 4.1. Build the network (30 points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R7ZLYqHoPwGg"
      },
      "outputs": [],
      "source": [
        "# build the supervised autoencoder network\n",
        "from keras.layers import *\n",
        "from keras import models\n",
        "\n",
        "input_img = Input(shape=(784,), name='input_img')\n",
        "\n",
        "# encoder network\n",
        "\n",
        "dropout = Dropout(0.1)(input_img)\n",
        "encode1 = Dense(350, activation='relu', kernel_regularizer=\"L2\")(dropout)\n",
        "encode2 = Dense(100, activation='relu')(encode1)\n",
        "dropout2 = Dropout(0.2)(encode2)\n",
        "encode3 = Dense(20, activation='relu')(dropout2)\n",
        "\n",
        "# The width of the bottleneck layer must be exactly 2.\n",
        "\n",
        "bottleneck = Dense(2, activation='relu')(encode3)\n",
        "\n",
        "# decoder network\n",
        "\n",
        "decode1 = Dense(20, activation='relu')(bottleneck)\n",
        "decode2 = Dense(100, activation='relu')(decode1)\n",
        "decode3 = Dense(350, activation='relu')(decode2)\n",
        "decode4 = Dense(784, activation='relu')(decode3)\n",
        "\n",
        "# build a classifier upon the bottleneck layer\n",
        "classifier1 = Dense(300, activation='relu', kernel_regularizer=\"L2\")(bottleneck)\n",
        "classifier2 = Dense(128, activation='relu')(classifier1)\n",
        "classifier3 = Dense(20, activation='relu')(classifier2)\n",
        "classifier4 = Dense(10, activation='softmax')(classifier3)\n",
        "\n",
        "# <Add more dense layers and regularizations...>\n",
        "# classifier3 = <the output of classifier network>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8TZHyOTPwGg"
      },
      "outputs": [],
      "source": [
        "# connect the input and the two outputs\n",
        "sae = models.Model(input_img, [decode4, classifier4])\n",
        "\n",
        "sae.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6XBoTxgFPwGh"
      },
      "outputs": [],
      "source": [
        "# print the network structure to a PDF file\n",
        "\n",
        "from IPython.display import SVG\n",
        "from keras.utils.vis_utils import model_to_dot, plot_model\n",
        "\n",
        "SVG(model_to_dot(sae, show_shapes=False).create(prog='dot', format='svg'))\n",
        "\n",
        "plot_model(\n",
        "    model=sae, show_shapes=False,\n",
        "    to_file='supervised_ae.pdf'\n",
        ")\n",
        "\n",
        "# you can find the file \"supervised_ae.pdf\" in the current directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHBa8fdtPwGh"
      },
      "source": [
        "### 4.2. Train the new model and tune the hyper-parameters\n",
        "\n",
        "The new model has multiple output. Thus we specify **multiple** loss functions and their weights. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pil1ijN0PwGh"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "sae.compile(loss=['mean_squared_error', 'categorical_crossentropy'],\n",
        "            loss_weights=[1, 0.5], # to be tuned\n",
        "            optimizer=optimizers.RMSprop(learning_rate=1E-3))\n",
        "\n",
        "history = sae.fit(x_tr, [x_tr, y_tr], \n",
        "                  batch_size=32, \n",
        "                  epochs=100, \n",
        "                  validation_data=(x_val, [x_val, y_val]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrJHxmgVPwGh"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(loss))\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4IzyvuKPwGh"
      },
      "source": [
        "### Question  (10 points)\n",
        "\n",
        "Do you think overfitting is happening? If yes, what can you do? Please make necessary changes to the supervised autoencoder network structure.\n",
        "\n",
        "You can use the new model without overfitting for the following sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uoeh22s12EEp"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "It does not seem like my model is overfitting. Validation error is definitely not\n",
        "decreasing smoothly but the overall trend is decreasing. It was originally overfitting a bit\n",
        "when I only used a few additional dense layers for encoding. But I tuned the model & \n",
        "added some dropout, regularization and a few extra layers and it seems to fit the data fine. \n",
        "\n",
        "I think I can improve the generalization (decrease overfitting) if I add some noise to\n",
        "the training data, so I'll make this change here and potentially increase dropout 0.05.\n",
        "\n",
        "I tried playing around with noising the data. Originally I was noising the x_tr, x_val\n",
        "and x_testing data but the validation accuracy was awful. It seems that the noised data\n",
        "uses MSE to optimize for image similarity rather than categorical-cross-entropy for classification. \n",
        "\n",
        "I figured the data may generalize better if I only changed the training data to have noise,\n",
        "but while the image reconstructions were much clearer, the classification was still worse.\n",
        "\n",
        "Noise = 0.1 --> val_acc = 0.926\n",
        "Noise = 0.0 --> val_acc = 0.9275\n",
        "'''\n",
        "\n",
        "# Noising data\n",
        "noise_factor = 0.0\n",
        "x_tr_noisy = x_tr + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_tr.shape)\n",
        "x_tr_noisy = np.clip(x_tr_noisy, 0, 1)\n",
        "\n",
        "# build the supervised autoencoder network\n",
        "from keras.layers import *\n",
        "from keras import models\n",
        "\n",
        "input_img = Input(shape=(784,), name='input_img')\n",
        "\n",
        "# encoder network\n",
        "\n",
        "dropout = Dropout(0.2)(input_img)\n",
        "encode1 = Dense(256, activation='relu', kernel_regularizer=\"L2\")(dropout)\n",
        "encode2 = Dense(100, activation='relu')(encode1)\n",
        "dropout2 = Dropout(0.25)(encode2)\n",
        "encode3 = Dense(20, activation='relu')(dropout2)\n",
        "\n",
        "# The width of the bottleneck layer must be exactly 2.\n",
        "\n",
        "bottleneck = Dense(2, activation='relu')(encode3)\n",
        "\n",
        "# decoder network\n",
        "\n",
        "decode1 = Dense(20, activation='relu')(bottleneck)\n",
        "decode2 = Dense(100, activation='relu')(decode1)\n",
        "decode3 = Dense(256, activation='relu')(decode2)\n",
        "decode4 = Dense(784, activation='relu')(decode3)\n",
        "\n",
        "# build a classifier upon the bottleneck layer\n",
        "classifier1 = Dense(256, activation='relu', kernel_regularizer=\"L2\")(bottleneck)\n",
        "classifier2 = Dense(128, activation='relu')(classifier1)\n",
        "classifier3 = Dense(20, activation='relu')(classifier2)\n",
        "classifier4 = Dense(10, activation='softmax')(classifier3)\n",
        "\n",
        "# Training Model\n",
        "sae = models.Model(input_img, [decode4, classifier4])\n",
        "\n",
        "from tensorflow.keras import optimizers\n",
        "\n",
        "sae.compile(loss=['mean_squared_error', 'categorical_crossentropy'],\n",
        "            loss_weights=[1, 0.5], # to be tuned\n",
        "            optimizer=optimizers.RMSprop(learning_rate=1E-3))\n",
        "\n",
        "history = sae.fit(x_tr_noisy, [x_tr_noisy, y_tr], \n",
        "                  batch_size=32, \n",
        "                  epochs=100, \n",
        "                  validation_data=(x_val, [x_val, y_val]))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(loss))\n",
        "\n",
        "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gBJlEPVlrKoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9UgknLLGPwGh"
      },
      "source": [
        "### 4.3. Visualize the reconstructed test images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQWjdq-sPwGh"
      },
      "outputs": [],
      "source": [
        "sae_output = sae.predict(x_test)[0].reshape((10000, 28, 28))\n",
        "\n",
        "ROW = 5\n",
        "COLUMN = 4\n",
        "\n",
        "x = sae_output\n",
        "fname = 'reconstruct_sae.pdf'\n",
        "\n",
        "fig, axes = plt.subplots(nrows=ROW, ncols=COLUMN, figsize=(8, 10))\n",
        "for ax, i in zip(axes.flat, np.arange(ROW*COLUMN)):\n",
        "    image = x[i].reshape(28, 28)\n",
        "    ax.imshow(image, cmap='gray')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(fname)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFS6yFBnPwGi"
      },
      "source": [
        "### 4.4. Visualize the low-dimensional features\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrQES8hFPwGi"
      },
      "outputs": [],
      "source": [
        "# build the encoder model\n",
        "sae_encoder = models.Model(input_img, bottleneck)\n",
        "sae_encoder.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jl77LBCIPwGi",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# extract test features\n",
        "encoded_test = sae_encoder.predict(x_test)\n",
        "print('Shape of encoded_test: ' + str(encoded_test.shape))\n",
        "\n",
        "colors = np.array(['r', 'g', 'b', 'm', 'c', 'k', 'y', 'purple', 'darkred', 'navy'])\n",
        "colors_test = colors[y_test]\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "fig = plt.figure(figsize=(8, 8))\n",
        "plt.scatter(encoded_test[:, 0], encoded_test[:, 1], s=10, c=colors_test, edgecolors=colors_test)\n",
        "plt.axis('off')\n",
        "plt.tight_layout()\n",
        "fname = 'sae_code.pdf'\n",
        "plt.savefig(fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sX7uwbePwGi"
      },
      "source": [
        "### 4.5. Are the learned low-dim features discriminative? (10 points)\n",
        "\n",
        "To find the answer, lets train a classifier on the training set (the extracted 2-dim features) and evaluation on the validation and test set.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJpZmHXLPwGi"
      },
      "outputs": [],
      "source": [
        "# extract 2D features from the training, validation, and test samples\n",
        "f_tr = sae_encoder.predict(x_tr)\n",
        "f_val = sae_encoder.predict(x_val)\n",
        "f_te = sae_encoder.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYeL97nrPwGi"
      },
      "outputs": [],
      "source": [
        "from keras.layers import *\n",
        "from keras import models\n",
        "\n",
        "input_feat = Input(shape=(2,))\n",
        "classifier1 = Dense(300, activation='relu', kernel_regularizer=\"L2\")(input_feat)\n",
        "classifier2 = Dense(128, activation='relu')(classifier1)\n",
        "classifier3 = Dense(20, activation='relu')(classifier2)\n",
        "output = Dense(10, activation='softmax')(classifier3)\n",
        "\n",
        "classifier = models.Model(input_feat, output)\n",
        "\n",
        "classifier.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "M5oAu1pwPwGi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "outputId": "96eeab0b-9b14-4058-c0fa-a590b91a7d15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "298/313 [===========================>..] - ETA: 0s - loss: 0.1905 - acc: 0.9518"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-72-d04e9c239592>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m                   metrics=['acc'])\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m history = classifier.fit(f_tr, y_tr, \n\u001b[0m\u001b[1;32m      6\u001b[0m                         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                         \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1683\u001b[0m                         ):\n\u001b[1;32m   1684\u001b[0m                             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1685\u001b[0;31m                             \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1686\u001b[0m                             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1687\u001b[0m                                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    895\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    924\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 926\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    927\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creation_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    141\u001b[0m       (concrete_function,\n\u001b[1;32m    142\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m--> 143\u001b[0;31m     return concrete_function._call_flat(\n\u001b[0m\u001b[1;32m    144\u001b[0m         filtered_flat_args, captured_inputs=concrete_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1755\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1756\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1757\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1758\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1759\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    379\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    382\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "classifier.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=optimizers.RMSprop(learning_rate=1E-4),\n",
        "                  metrics=['acc'])\n",
        "\n",
        "history = classifier.fit(f_tr, y_tr, \n",
        "                        batch_size=32, \n",
        "                        epochs=30, \n",
        "                        validation_data=(f_val, y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF6gznU0PwGi"
      },
      "source": [
        "#### Remark: (10 points)\n",
        "\n",
        "The validation accuracy must be above 90%. It means the low-dim features learned by the supervised autoencoder are very effective."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "ETJ_shAxPwGj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cbd488a-edb4-40e7-9ba2-690734bbf4ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.2832 - acc: 0.9328\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.28317609429359436, 0.9327999949455261]"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ],
      "source": [
        "# evaluate your model on the never-seen-before test data\n",
        "# write your code here:\n",
        "classifier.evaluate(f_te, y_test_vec)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
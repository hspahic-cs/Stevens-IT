{"cells":[{"cell_type":"markdown","metadata":{"id":"CERWeJIMPwGY"},"source":["# Assignment 4: Build a Supervised Autoencoder.\n","\n","### Name: [Your-Name?]\n","\n","### Due Date: Tuesday 5/2/2023 11:59PM"]},{"cell_type":"markdown","metadata":{"id":"p5qGf_LLPwGZ"},"source":["\n","PCA and the standard autoencoder are unsupervised dimensionality reduction methods, and their learned features are not discriminative. If you build a classifier upon the low-dimenional features extracted by PCA and autoencoder, you will find the classification accuracy very poor.\n","\n","Linear discriminant analysis (LDA) is a traditionally supervised dimensionality reduction method for learning low-dimensional features which are highly discriminative. Likewise, can we extend autoencoder to supervised leanring?\n","\n"]},{"cell_type":"markdown","metadata":{"id":"SIFKjnABPwGZ"},"source":["**You are required to build and train a supervised autoencoder look like the following.** You are required to add other layers properly to alleviate overfitting.\n","\n","\n","![Network Structure](https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM5/supervised_ae.png?raw=true \"NetworkStructure\")\n"]},{"cell_type":"markdown","metadata":{"id":"fNohSFbFPwGa"},"source":["## 0. You will do the following:\n","\n","1. Build a standard dense autoencoder, visual the low-dim features and the reconstructions, and evaluate whether the learned low-dim features are discriminative.\n","\n","2. Repeat the above process by training a supervised autoencoder.\n"]},{"cell_type":"markdown","metadata":{"id":"ziHiYnJwPwGa"},"source":["## 1. Data preparation"]},{"cell_type":"markdown","metadata":{"id":"TTcBJb-WPwGa"},"source":["### 1.1. Load data\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mZ8T6_AxPwGa"},"outputs":[],"source":["from keras.datasets import mnist\n","\n","(x_train, y_train), (x_test, y_test) = mnist.load_data()\n","x_train = x_train.reshape(60000, 28*28).astype('float32') / 255.\n","x_test = x_test.reshape(10000, 28*28).astype('float32') / 255.\n","\n","print('Shape of x_train: ' + str(x_train.shape)) \n","print('Shape of x_test: ' + str(x_test.shape))\n","print('Shape of y_train: ' + str(y_train.shape))\n","print('Shape of y_test: ' + str(y_test.shape))"]},{"cell_type":"markdown","metadata":{"id":"1rLwXo0yPwGb"},"source":["### 1.2. One-hot encode the labels\n","\n","In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n","\n","1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n","\n","2. Apply the function to ```y_train``` and ```y_test```."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YyI16V84PwGb"},"outputs":[],"source":["import numpy as np\n","\n","def to_one_hot(y, num_class=10):\n","    results = np.zeros((len(y), num_class))\n","    for i, label in enumerate(y):\n","        results[i, label] = 1.\n","    return results\n","\n","y_train_vec = to_one_hot(y_train)\n","y_test_vec = to_one_hot(y_test)\n","\n","print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n","print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n","\n","print(y_train[0])\n","print(y_train_vec[0])"]},{"cell_type":"markdown","metadata":{"id":"Zbfl7Um8PwGc"},"source":["### 1.3. Randomly partition the training set to training and validation sets\n","\n","Randomly partition the 60K training samples to 2 sets:\n","* a training set containing 10K samples;\n","* a validation set containing 50K samples. (You can use only 10K to save time.)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fnjlaZzuPwGc"},"outputs":[],"source":["rand_indices = np.random.permutation(60000)\n","train_indices = rand_indices[0:10000]\n","valid_indices = rand_indices[10000:20000]\n","\n","x_val = x_train[valid_indices, :]\n","y_val = y_train_vec[valid_indices, :]\n","\n","x_tr = x_train[train_indices, :]\n","y_tr = y_train_vec[train_indices, :]\n","\n","print('Shape of x_tr: ' + str(x_tr.shape))\n","print('Shape of y_tr: ' + str(y_tr.shape))\n","print('Shape of x_val: ' + str(x_val.shape))\n","print('Shape of y_val: ' + str(y_val.shape))"]},{"cell_type":"markdown","metadata":{"id":"cVjMusdlPwGc"},"source":["## 2. Build an unsupervised  autoencoder and tune its hyper-parameters\n","\n","1. Build a dense autoencoder model\n","2. Your encoder should contain 3 dense layers and 1 bottlenect layer with 2 as  output size. \n","3. Your decoder should contain 4 dense layers with 784 as output size.\n","4. You can choose different number of hidden units in dense layers.\n","5. Do not add other layers (no activation layers), you may add them in later sections.\n","6. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n","    * Do NOT use test data for hyper-parameter tuning!!!\n","    \n","7. Try to achieve a validation loss as low as possible.\n","8. Evaluate the model on the test set.\n","9. Visualize the low-dim features and reconstructions.\n"]},{"cell_type":"markdown","metadata":{"id":"XbKlAcMFPwGc"},"source":["### 2.1. Build the model (20 points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dL5MH6UlPwGc"},"outputs":[],"source":["from keras.layers import *\n","from keras import models\n","\n","input_img = Input(shape=(784,), name='input_img')\n","\n","encode1 =  <add a dense layer taking input_img as input>\n","<Add more layers...>\n","\n","bottleneck = <the output of encoder network>\n","\n","decode1 =  <add a dense layer taking bottleneck as input>\n","<Add more layers...>\n","decode4 = <the output of decoder network>\n","\n","\n","ae = models.Model(input_img, decode4)\n","\n","ae.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mq0XpDhTPwGd"},"outputs":[],"source":["# print the network structure to a PDF file\n","\n","from IPython.display import SVG\n","from keras.utils.vis_utils import model_to_dot, plot_model\n","\n","SVG(model_to_dot(ae, show_shapes=False).create(prog='dot', format='svg'))\n","\n","plot_model(\n","    model=ae, show_shapes=False,\n","    to_file='unsupervised_ae.pdf'\n",")\n","\n","# you can find the file \"unsupervised_ae.pdf\" in the current directory."]},{"cell_type":"markdown","metadata":{"id":"SSf8i8QHPwGd"},"source":["### 2.2. Train the model and tune the hyper-parameters (5 points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"viuWz8tWPwGd"},"outputs":[],"source":["from tensorflow.keras import optimizers\n","\n","learning_rate = 1E-3 # to be tuned!\n","\n","ae.compile(loss='mean_squared_error',\n","           optimizer=optimizers.RMSprop(learning_rate=learning_rate))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0DeL_ZDzPwGd"},"outputs":[],"source":["history = ae.fit(x_tr, x_tr, \n","                 batch_size=128, \n","                 epochs=100, \n","                 validation_data=(x_val, x_val))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sTeRNc6ePwGe"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(len(loss))\n","\n","plt.plot(epochs, loss, 'bo', label='Training Loss')\n","plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"jGCqJmkZPwGe"},"source":["### 2.3. Visualize the reconstructed test images (5 points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sk0TixTcPwGe"},"outputs":[],"source":["ae_output = ae.predict(x_test).reshape((10000, 28, 28))\n","\n","ROW = 5\n","COLUMN = 4\n","\n","x = ae_output\n","fname = 'reconstruct_ae.pdf'\n","\n","fig, axes = plt.subplots(nrows=ROW, ncols=COLUMN, figsize=(8, 10))\n","for ax, i in zip(axes.flat, np.arange(ROW*COLUMN)):\n","    image = x[i].reshape(28, 28)\n","    ax.imshow(image, cmap='gray')\n","    ax.axis('off')\n","\n","plt.tight_layout()\n","plt.savefig(fname)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"XC9oKRltPwGe"},"source":["### 2.4. Evaluate the model on the test set\n","\n","Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"viaZ0vGXPwGe"},"outputs":[],"source":["loss = ae.evaluate(x_test, x_test)\n","print('loss = ' + str(loss))"]},{"cell_type":"markdown","metadata":{"id":"ZiH0YYflPwGe"},"source":["### 2.5. Visualize the low-dimensional features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EjNJXDsIPwGf"},"outputs":[],"source":["# build the encoder network\n","ae_encoder = models.Model(input_img, bottleneck)\n","ae_encoder.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8KufyF8uPwGf"},"outputs":[],"source":["# extract low-dimensional features from the test data\n","encoded_test = ae_encoder.predict(x_test)\n","print('Shape of encoded_test: ' + str(encoded_test.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RdA-flR5PwGf"},"outputs":[],"source":["colors = np.array(['r', 'g', 'b', 'm', 'c', 'k', 'y', 'purple', 'darkred', 'navy'])\n","colors_test = colors[y_test]\n","\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","fig = plt.figure(figsize=(8, 8))\n","plt.scatter(encoded_test[:, 0], encoded_test[:, 1], s=10, c=colors_test, edgecolors=colors_test)\n","plt.axis('off')\n","plt.tight_layout()\n","fname = 'ae_code.pdf'\n","plt.savefig(fname)"]},{"cell_type":"markdown","metadata":{"id":"vr0UMMcAPwGf"},"source":["#### Remark:\n","\n","Judging from the visualization, the low-dim features seems not discriminative, as 2D features from different classes are mixed. Let quantatively find out whether they are discriminative."]},{"cell_type":"markdown","metadata":{"id":"HjRVaPmvPwGf"},"source":["## 3. Are the learned low-dim features discriminative? (10 points)\n","\n","To find the answer, lets train a classifier on the training set (the extracted 2-dim features) and evaluation on the test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QPvkWFAvPwGf"},"outputs":[],"source":["# extract the 2D features from the training, validation, and test samples\n","f_tr = ae_encoder.predict(x_tr)\n","f_val = ae_encoder.predict(x_val)\n","f_te = ae_encoder.predict(x_test)\n","\n","print('Shape of f_tr: ' + str(f_tr.shape))\n","print('Shape of f_te: ' + str(f_te.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nsp4Gbr9PwGf"},"outputs":[],"source":["from keras.layers import Dense, Input\n","from keras import models\n","\n","input_feat = Input(shape=(2,))\n","\n","hidden1 = Dense(128, activation='relu')(input_feat)\n","hidden2 = Dense(128, activation='relu')(hidden1)\n","output = Dense(10, activation='softmax')(hidden2)\n","\n","classifier = models.Model(input_feat, output)\n","\n","classifier.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qd2igsB6PwGg"},"outputs":[],"source":["classifier.compile(loss='categorical_crossentropy',\n","                  optimizer=optimizers.RMSprop(learning_rate=1E-4),\n","                  metrics=['acc'])\n","\n","history = classifier.fit(f_tr, y_tr, \n","                        batch_size=32, \n","                        epochs=30, \n","                        validation_data=(f_val, y_val))"]},{"cell_type":"markdown","metadata":{"id":"Kz_ewffqPwGg"},"source":["### Conclusion\n","\n","Using the 2D features, the validation accuracy is 60~70%. Recall that using the original data, the accuracy is about 97%. Obviously, the 2D features are not very discriminative.\n","\n","We are going to build a supervised autoencode model for learning low-dimensional discriminative features."]},{"cell_type":"markdown","metadata":{"id":"U2nW56WAPwGg"},"source":["## 4. Build a supervised autoencoder model\n","\n","\n","**You are required to build and train a supervised autoencoder look like the following.** (Not necessary the same. You can use convolutional layers as well.) You are required to add other layers properly to alleviate overfitting.\n","\n","\n","![Network Structure](https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM5/supervised_ae.png?raw=true \"NetworkStructure\")\n"]},{"cell_type":"markdown","metadata":{"id":"ec-KwvH9PwGg"},"source":["### 4.1. Build the network (30 points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R7ZLYqHoPwGg"},"outputs":[],"source":["# build the supervised autoencoder network\n","from keras.layers import *\n","from keras import models\n","\n","input_img = Input(shape=(784,), name='input_img')\n","\n","# encoder network\n","\n","encode1 = <add a dense layer taking input_img as input>\n","<Add more layers...>\n","\n","# The width of the bottleneck layer must be exactly 2.\n","\n","bottleneck = <the output of encoder network>\n","\n","# decoder network\n","decode1 =  <add a dense layer taking bottleneck as input>\n","<Add more layers...>\n","decode4 = <the output of decoder network>\n","\n","\n","# build a classifier upon the bottleneck layer\n","classifier1 = <add a dense layer taking bottleneck as input>\n","<Add more dense layers and regularizations...>\n","classifier3 = <the output of classifier network>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z8TZHyOTPwGg"},"outputs":[],"source":["# connect the input and the two outputs\n","sae = models.Model(input_img, [decode4, classifier3])\n","\n","sae.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6XBoTxgFPwGh"},"outputs":[],"source":["# print the network structure to a PDF file\n","\n","from IPython.display import SVG\n","from keras.utils.vis_utils import model_to_dot, plot_model\n","\n","SVG(model_to_dot(sae, show_shapes=False).create(prog='dot', format='svg'))\n","\n","plot_model(\n","    model=sae, show_shapes=False,\n","    to_file='supervised_ae.pdf'\n",")\n","\n","# you can find the file \"supervised_ae.pdf\" in the current directory."]},{"cell_type":"markdown","metadata":{"id":"VHBa8fdtPwGh"},"source":["### 4.2. Train the new model and tune the hyper-parameters\n","\n","The new model has multiple output. Thus we specify **multiple** loss functions and their weights. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pil1ijN0PwGh"},"outputs":[],"source":["from tensorflow.keras import optimizers\n","\n","sae.compile(loss=['mean_squared_error', 'categorical_crossentropy'],\n","            loss_weights=[1, 0.5], # to be tuned\n","            optimizer=optimizers.RMSprop(learning_rate=1E-3))\n","\n","history = sae.fit(x_tr, [x_tr, y_tr], \n","                  batch_size=32, \n","                  epochs=100, \n","                  validation_data=(x_val, [x_val, y_val]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YrJHxmgVPwGh"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(len(loss))\n","\n","plt.plot(epochs, loss, 'bo', label='Training Loss')\n","plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"P4IzyvuKPwGh"},"source":["### Question  (10 points)\n","\n","Do you think overfitting is happening? If yes, what can you do? Please make necessary changes to the supervised autoencoder network structure.\n","\n","You can use the new model without overfitting for the following sections."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uoeh22s12EEp"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"9UgknLLGPwGh"},"source":["### 4.3. Visualize the reconstructed test images"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mQWjdq-sPwGh"},"outputs":[],"source":["sae_output = sae.predict(x_test)[0].reshape((10000, 28, 28))\n","\n","ROW = 5\n","COLUMN = 4\n","\n","x = sae_output\n","fname = 'reconstruct_sae.pdf'\n","\n","fig, axes = plt.subplots(nrows=ROW, ncols=COLUMN, figsize=(8, 10))\n","for ax, i in zip(axes.flat, np.arange(ROW*COLUMN)):\n","    image = x[i].reshape(28, 28)\n","    ax.imshow(image, cmap='gray')\n","    ax.axis('off')\n","\n","plt.tight_layout()\n","plt.savefig(fname)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"nFS6yFBnPwGi"},"source":["### 4.4. Visualize the low-dimensional features\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OrQES8hFPwGi"},"outputs":[],"source":["# build the encoder model\n","sae_encoder = models.Model(input_img, bottleneck)\n","sae_encoder.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jl77LBCIPwGi","scrolled":true},"outputs":[],"source":["# extract test features\n","encoded_test = sae_encoder.predict(x_test)\n","print('Shape of encoded_test: ' + str(encoded_test.shape))\n","\n","colors = np.array(['r', 'g', 'b', 'm', 'c', 'k', 'y', 'purple', 'darkred', 'navy'])\n","colors_test = colors[y_test]\n","\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","fig = plt.figure(figsize=(8, 8))\n","plt.scatter(encoded_test[:, 0], encoded_test[:, 1], s=10, c=colors_test, edgecolors=colors_test)\n","plt.axis('off')\n","plt.tight_layout()\n","fname = 'sae_code.pdf'\n","plt.savefig(fname)"]},{"cell_type":"markdown","metadata":{"id":"2sX7uwbePwGi"},"source":["### 4.5. Are the learned low-dim features discriminative? (10 points)\n","\n","To find the answer, lets train a classifier on the training set (the extracted 2-dim features) and evaluation on the validation and test set.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qJpZmHXLPwGi"},"outputs":[],"source":["# extract 2D features from the training, validation, and test samples\n","f_tr = sae_encoder.predict(x_tr)\n","f_val = sae_encoder.predict(x_val)\n","f_te = sae_encoder.predict(x_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JYeL97nrPwGi"},"outputs":[],"source":["# build a classifier which takes the 2D features as input\n","from keras.layers import *\n","from keras import models\n","\n","input_feat = Input(shape=(2,))\n","\n","<build a classifier which takes input_feat as input>\n","output = <output of the classifier network>\n","\n","classifier = models.Model(input_feat, output)\n","\n","classifier.summary()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M5oAu1pwPwGi"},"outputs":[],"source":["classifier.compile(loss='categorical_crossentropy',\n","                  optimizer=optimizers.RMSprop(learning_rate=1E-4),\n","                  metrics=['acc'])\n","\n","history = classifier.fit(f_tr, y_tr, \n","                        batch_size=32, \n","                        epochs=30, \n","                        validation_data=(f_val, y_val))"]},{"cell_type":"markdown","metadata":{"id":"RF6gznU0PwGi"},"source":["#### Remark: (10 points)\n","\n","The validation accuracy must be above 90%. It means the low-dim features learned by the supervised autoencoder are very effective."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ETJ_shAxPwGj"},"outputs":[],"source":["# evaluate your model on the never-seen-before test data\n","# write your code here:\n","..."]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoUbYLVxMNBw"
      },
      "source": [
        "# HM1: Logistic Regression.\n",
        "\n",
        "### Name: Harris Spahic\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUOQqQHBMNBy"
      },
      "source": [
        "#### For this assignment, you will build 6 models. You need to train Logistic Regression/Regularized Logistic Regression each with Batch Gradient Descent, Stochastic Gradient Descent and Mini Batch Gradient Descent. Also, you should plot their objective values versus epochs and compare their training and testing accuracy. You will need to tune the parameters a little bit to obtain reasonable results.\n",
        "\n",
        "#### You do not have to follow the following procedure. You may implement your own functions and methods, but you need to show your results and plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HSfPCVZSMNBz"
      },
      "outputs": [],
      "source": [
        "# Load Packages\n",
        "import numpy\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sW2RtUDNMNB1"
      },
      "source": [
        "# 1. Data processing\n",
        "\n",
        "- Download the Breast Cancer dataset from canvas or from https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)\n",
        "- Load the data.\n",
        "- Preprocess the data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1zAM7gaMNB1"
      },
      "source": [
        "## 1.1. Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W2r6QeA0MNB1"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"data-1.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bNdsIc5MNB2"
      },
      "source": [
        "## 1.2 Examine and clean data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdUfNb85MNB2"
      },
      "outputs": [],
      "source": [
        "# Some columns may not be useful for the model (For example, the first column contains ID number which may be irrelavant). \n",
        "# You need to get rid of the ID number feature.\n",
        "# Also you should transform target labels in the second column from 'B' and 'M' to 1 and -1.\n",
        "data.head()\n",
        "data = data.drop(columns = [\"id\", \"Unnamed: 32\"])\n",
        "\n",
        "def alter_diagnosis(row):\n",
        "  if(row.diagnosis == 'M'):\n",
        "    row.diagnosis = -1\n",
        "  else:\n",
        "    row.diagnosis = 1\n",
        "  return row\n",
        "\n",
        "data = data.apply(alter_diagnosis, axis=\"columns\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2IjOXpHAoGC",
        "outputId": "8809baad-ecf6-4db1-be44-25ee9fda45ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
            "0           -1        17.99         10.38          122.80     1001.0   \n",
            "1           -1        20.57         17.77          132.90     1326.0   \n",
            "2           -1        19.69         21.25          130.00     1203.0   \n",
            "3           -1        11.42         20.38           77.58      386.1   \n",
            "4           -1        20.29         14.34          135.10     1297.0   \n",
            "..         ...          ...           ...             ...        ...   \n",
            "564         -1        21.56         22.39          142.00     1479.0   \n",
            "565         -1        20.13         28.25          131.20     1261.0   \n",
            "566         -1        16.60         28.08          108.30      858.1   \n",
            "567         -1        20.60         29.33          140.10     1265.0   \n",
            "568          1         7.76         24.54           47.92      181.0   \n",
            "\n",
            "     smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
            "0            0.11840           0.27760         0.30010              0.14710   \n",
            "1            0.08474           0.07864         0.08690              0.07017   \n",
            "2            0.10960           0.15990         0.19740              0.12790   \n",
            "3            0.14250           0.28390         0.24140              0.10520   \n",
            "4            0.10030           0.13280         0.19800              0.10430   \n",
            "..               ...               ...             ...                  ...   \n",
            "564          0.11100           0.11590         0.24390              0.13890   \n",
            "565          0.09780           0.10340         0.14400              0.09791   \n",
            "566          0.08455           0.10230         0.09251              0.05302   \n",
            "567          0.11780           0.27700         0.35140              0.15200   \n",
            "568          0.05263           0.04362         0.00000              0.00000   \n",
            "\n",
            "     symmetry_mean  ...  radius_worst  texture_worst  perimeter_worst  \\\n",
            "0           0.2419  ...        25.380          17.33           184.60   \n",
            "1           0.1812  ...        24.990          23.41           158.80   \n",
            "2           0.2069  ...        23.570          25.53           152.50   \n",
            "3           0.2597  ...        14.910          26.50            98.87   \n",
            "4           0.1809  ...        22.540          16.67           152.20   \n",
            "..             ...  ...           ...            ...              ...   \n",
            "564         0.1726  ...        25.450          26.40           166.10   \n",
            "565         0.1752  ...        23.690          38.25           155.00   \n",
            "566         0.1590  ...        18.980          34.12           126.70   \n",
            "567         0.2397  ...        25.740          39.42           184.60   \n",
            "568         0.1587  ...         9.456          30.37            59.16   \n",
            "\n",
            "     area_worst  smoothness_worst  compactness_worst  concavity_worst  \\\n",
            "0        2019.0           0.16220            0.66560           0.7119   \n",
            "1        1956.0           0.12380            0.18660           0.2416   \n",
            "2        1709.0           0.14440            0.42450           0.4504   \n",
            "3         567.7           0.20980            0.86630           0.6869   \n",
            "4        1575.0           0.13740            0.20500           0.4000   \n",
            "..          ...               ...                ...              ...   \n",
            "564      2027.0           0.14100            0.21130           0.4107   \n",
            "565      1731.0           0.11660            0.19220           0.3215   \n",
            "566      1124.0           0.11390            0.30940           0.3403   \n",
            "567      1821.0           0.16500            0.86810           0.9387   \n",
            "568       268.6           0.08996            0.06444           0.0000   \n",
            "\n",
            "     concave points_worst  symmetry_worst  fractal_dimension_worst  \n",
            "0                  0.2654          0.4601                  0.11890  \n",
            "1                  0.1860          0.2750                  0.08902  \n",
            "2                  0.2430          0.3613                  0.08758  \n",
            "3                  0.2575          0.6638                  0.17300  \n",
            "4                  0.1625          0.2364                  0.07678  \n",
            "..                    ...             ...                      ...  \n",
            "564                0.2216          0.2060                  0.07115  \n",
            "565                0.1628          0.2572                  0.06637  \n",
            "566                0.1418          0.2218                  0.07820  \n",
            "567                0.2650          0.4087                  0.12400  \n",
            "568                0.0000          0.2871                  0.07039  \n",
            "\n",
            "[569 rows x 31 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GP8tfArVMNB2"
      },
      "source": [
        "## 1.3. Partition to training and testing sets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_gp5tq3OMNB3"
      },
      "outputs": [],
      "source": [
        "# You can partition using 80% training data and 20% testing data. It is a commonly used ratio in machine learning.\n",
        "features = list(data.columns.values)\n",
        "features.remove(\"diagnosis\")\n",
        "X = data.loc[:, features]\n",
        "y = data.loc[:, [\"diagnosis\"]]\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icX3ZsOzMNB3"
      },
      "source": [
        "## 1.4. Feature scaling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKQlqlT_MNB3"
      },
      "source": [
        "Use the standardization to transform both training and test features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PidGco-AMNB3",
        "outputId": "70a0f39f-1caa-4d29-b49e-c49e7bcd0b29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test mean = \n",
            "radius_mean               -0.011530\n",
            "texture_mean              -0.049648\n",
            "perimeter_mean            -0.024278\n",
            "area_mean                 -0.030448\n",
            "smoothness_mean           -0.086311\n",
            "compactness_mean          -0.161125\n",
            "concavity_mean            -0.173081\n",
            "concave points_mean       -0.118854\n",
            "symmetry_mean             -0.164165\n",
            "fractal_dimension_mean    -0.111343\n",
            "radius_se                 -0.041021\n",
            "texture_se                -0.056817\n",
            "perimeter_se              -0.083738\n",
            "area_se                   -0.037372\n",
            "smoothness_se              0.101092\n",
            "compactness_se            -0.090339\n",
            "concavity_se              -0.077154\n",
            "concave points_se          0.026614\n",
            "symmetry_se               -0.067501\n",
            "fractal_dimension_se      -0.049114\n",
            "radius_worst              -0.016442\n",
            "texture_worst             -0.056242\n",
            "perimeter_worst           -0.045007\n",
            "area_worst                -0.030742\n",
            "smoothness_worst          -0.089337\n",
            "compactness_worst         -0.183273\n",
            "concavity_worst           -0.172896\n",
            "concave points_worst      -0.097847\n",
            "symmetry_worst            -0.155435\n",
            "fractal_dimension_worst   -0.104520\n",
            "dtype: float64\n",
            "test std = \n",
            "radius_mean                0.878722\n",
            "texture_mean               0.844264\n",
            "perimeter_mean             0.865596\n",
            "area_mean                  0.833381\n",
            "smoothness_mean            0.938096\n",
            "compactness_mean           0.740519\n",
            "concavity_mean             0.732034\n",
            "concave points_mean        0.800389\n",
            "symmetry_mean              0.863104\n",
            "fractal_dimension_mean     0.908649\n",
            "radius_se                  0.809816\n",
            "texture_se                 0.797983\n",
            "perimeter_se               0.742678\n",
            "area_se                    0.715284\n",
            "smoothness_se              1.281863\n",
            "compactness_se             0.923997\n",
            "concavity_se               0.741775\n",
            "concave points_se          0.972605\n",
            "symmetry_se                0.783677\n",
            "fractal_dimension_se       0.869523\n",
            "radius_worst               0.894132\n",
            "texture_worst              0.812853\n",
            "perimeter_worst            0.861239\n",
            "area_worst                 0.861553\n",
            "smoothness_worst           0.906860\n",
            "compactness_worst          0.754433\n",
            "concavity_worst            0.784418\n",
            "concave points_worst       0.822799\n",
            "symmetry_worst             0.807468\n",
            "fractal_dimension_worst    0.942904\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Standardization\n",
        "import numpy\n",
        "\n",
        "# calculate mu and sig using the training set\n",
        "d = x_train.shape[1]\n",
        "mu = numpy.mean(x_train, axis=0)\n",
        "sig = numpy.std(x_train, axis=0)\n",
        "\n",
        "# transform the training features\n",
        "x_train = (x_train - mu) / (sig + 1E-6)\n",
        "\n",
        "# transform the test features\n",
        "x_test = (x_test - mu) / (sig + 1E-6)\n",
        "\n",
        "print('test mean = ')\n",
        "print(numpy.mean(x_test, axis=0))\n",
        "\n",
        "print('test std = ')\n",
        "print(numpy.std(x_test, axis=0))\n",
        "\n",
        "x_train, y_train, x_test, y_test = x_train.to_numpy(), y_train.to_numpy(), x_test.to_numpy(), y_test.to_numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRWF1FJNMNB4"
      },
      "source": [
        "# 2.  Logistic Regression Model\n",
        "\n",
        "The objective function is $Q (w; X, y) = \\frac{1}{n} \\sum_{i=1}^n \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $.\n",
        "\n",
        "When $\\lambda = 0$, the model is a regular logistic regression and when $\\lambda > 0$, it essentially becomes a regularized logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjCGZRu3MNB4"
      },
      "outputs": [],
      "source": [
        "# Calculate the objective function value, or loss\n",
        "# Inputs:\n",
        "#     w: weight: d-by-1 matrix\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: regularization parameter: scalar\n",
        "# Return:\n",
        "#     objective function value, or loss (scalar)\n",
        "\n",
        "def objective(w, x, y, lam):\n",
        "    assert(x.shape[0] == y.shape[0])\n",
        "    assert(x.shape[1] == w.shape[0])\n",
        "\n",
        "    sum = 0; n = x.shape[0]\n",
        "    for i in range(n):\n",
        "      z = numpy.matmul(numpy.multiply(y[i,:], x[i, :]), w)\n",
        "      result = numpy.log(1 + numpy.exp(-z))\n",
        "      sum += result\n",
        "\n",
        "    bias = (lam / 2) * numpy.linalg.norm(w)\n",
        "    return sum / n + bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BK_8aPz8MNB5"
      },
      "source": [
        "# 3. Numerical optimization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrN99Ir1MNB5"
      },
      "source": [
        "## 3.1. Gradient descent\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeDpuRGvMNB5"
      },
      "source": [
        "The gradient at $w$ for regularized logistic regression is  $g = - \\frac{1}{n} \\sum_{i=1}^n \\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aB3QP7XMNB5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ecb855d-7a8e-410d-8612-eb71d65d39d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.13940677 0.87052781 1.15847    1.12395891 0.90443253 1.14821285\n",
            " 1.21194901 1.25241669 0.88872401 0.64285182 1.01836943 0.53928533\n",
            " 1.02235917 0.99033007 0.49285693 0.91750294 0.84703261 0.98954383\n",
            " 0.57139691 0.71992744 1.18492835 0.91531349 1.1996699  1.15143909\n",
            " 0.93854961 1.12378615 1.1748716  1.26587501 0.93253374 0.91188311]\n"
          ]
        }
      ],
      "source": [
        "# Calculate the gradient\n",
        "# Inputs:\n",
        "#     w: weight: d-by-1 matrix\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: regularization parameter: scalar\n",
        "# Return:\n",
        "#     g: gradient: d-by-1 matrix\n",
        "\n",
        "def gradient(w, x, y, lam):\n",
        "    assert(x.shape[0] == y.shape[0])\n",
        "    assert(x.shape[1] == w.shape[0])\n",
        "\n",
        "    sum = 0; n = x.shape[0]\n",
        "    for i in range(n):\n",
        "      z = numpy.matmul(numpy.multiply(y[i], x[i]), w)\n",
        "      numerator = numpy.multiply(y[i], x[i])\n",
        "      sum -= numerator / (1 + numpy.exp(z))\n",
        "    \n",
        "    sum = sum/n\n",
        "    return sum + lam * w\n",
        "\n",
        "test_weights = numpy.ones(x_train.shape[1])\n",
        "print(gradient(test_weights, x_train, y_train, 0.5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Betyc2XMNB5"
      },
      "outputs": [],
      "source": [
        "# Gradient descent for solving logistic regression\n",
        "# You will need to do iterative processes (loops) to obtain optimal weights in this function\n",
        "\n",
        "# Inputs:\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: scalar, the regularization parameter\n",
        "#     learning_rate: scalar\n",
        "#     w: weights: d-by-1 matrix, initialization of w\n",
        "#     max_epoch: integer, the maximal epochs\n",
        "# Return:\n",
        "#     w: weights: d-by-1 matrix, the solution\n",
        "#     objvals: a record of each epoch's objective value\n",
        "\n",
        "def gradient_descent(x, y, lam, learning_rate, w, max_epoch=100):\n",
        "    objvals = []\n",
        "    \n",
        "    for epoch in range(max_epoch):\n",
        "      obj = objective(w, x, y, lam)\n",
        "      w -= gradient(w, x, y, lam) * learning_rate\n",
        "\n",
        "      objvals.append(obj)\n",
        "\n",
        "    return w, objvals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q596yRUbMNB5"
      },
      "source": [
        "Use gradient_descent function to obtain your optimal weights and a list of objective values over each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qrEAWoaMNB6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "584898d2-ae6b-492c-a87f-98458021f3e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14.297006648784398, 13.487196995212255, 12.680314417382805, 11.877529688682587, 11.08026771067247, 10.289585321228493, 9.505916109609336, 8.729583545229339, 7.961259791765186, 7.202209889972502, 6.454555042442677, 5.721607110714859, 5.008154179556991, 4.320756505300139, 3.6681026986839815, 3.061513353415488, 2.5158657802491504, 2.0466014010750095, 1.661304152690012, 1.3563052851493502, 1.1207198911075398, 0.9390293448585079, 0.7973773189093984, 0.6853054391565628, 0.5960466340991138, 0.5249338689093612, 0.46834866826636956, 0.42337017982468056, 0.3875755049687151, 0.358887651803037, 0.3355830477258795, 0.31633416975228706, 0.3001751192454097, 0.2864144202391631, 0.2745502582929078, 0.26420953366798616, 0.25510842360303604, 0.24702713003508658, 0.2397931533599093, 0.23326969330876146, 0.2273472841981949, 0.22193760035976381, 0.21696879473671352, 0.21238195725427442, 0.20812840604292784, 0.2041676038977612, 0.20046554657772966, 0.19699350859680445, 0.1937270609582381, 0.1906452966986202, 0.18773021605901286, 0.1849662349784669, 0.18233978945119675, 0.17983901488386486, 0.1774534845175904, 0.17517399467396305, 0.17299238736575037, 0.17090140291696074, 0.16889455683687446, 0.16696603641619295, 0.16511061345455155, 0.16332357025687583, 0.16160063660287702, 0.1599379358378619, 0.15833193858264855, 0.15677942283736898, 0.15527743947463857, 0.15382328229437894, 0.1524144619550033, 0.1510486832109805, 0.1497238249806322, 0.14843792284473967, 0.14718915363957424, 0.14597582185996916, 0.14479634763116492, 0.14364925604402531, 0.14253316767818858, 0.14144679016285597, 0.14038891064606185, 0.1393583890611423, 0.13835415209425284, 0.1373751877696512, 0.13642054058043415, 0.13548930710179533, 0.13458063203191684, 0.13369370461251842, 0.13282775538704822, 0.13198205325964693, 0.13115590282247747, 0.1303486419228743, 0.12955963944513899, 0.12878829328473326, 0.1280340284951831, 0.1272962955902491, 0.12657456898587133, 0.12586834556813062, 0.125177143374971, 0.1245005003807674, 0.12383797337399735, 0.12318913691930937]\n"
          ]
        }
      ],
      "source": [
        "# Train logistic regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "\n",
        "lgr_weights = numpy.ones(x_train.shape[1])\n",
        "lgr_weights, lgr_objvals = gradient_descent(x_train, y_train, 0, 0.1, lgr_weights, max_epoch = 100)\n",
        "print(lgr_objvals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HkA_V8-GMNB6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8996c68d-b064-4431-de69-eca9b14dc05e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[15.666313042547314, 14.011320638739262, 12.444246935761353, 10.96278546091547, 9.564962798618009, 8.248602625795291, 7.011982294271195, 5.855218642061025, 4.781782954001458, 3.8002586339846856, 2.926046955091574, 2.1830085909598256, 1.600947568062428, 1.1932980160798274, 0.9334798256306005, 0.7711071291917784, 0.6672895214227277, 0.5998210442134725, 0.5548868001261484, 0.5237224600394406, 0.5011728935520138, 0.48426294806374803, 0.4712153322095792, 0.4609174057212312, 0.45264121477484864, 0.4458918780942649, 0.44032179159436846, 0.4356798822105138, 0.43178035348622, 0.4284827497754222, 0.42567885866258687, 0.4232838883057767, 0.4212303943690968, 0.4194640176176768, 0.4179404378227164, 0.4166231590620384, 0.4154818724020859, 0.414491225633583, 0.4136298842303122, 0.4128798037369982, 0.4122256579349839, 0.4116543834962657, 0.4111548130427483, 0.4107173762800135, 0.4103338542912893, 0.40999717589982915, 0.4097012477341495, 0.40944081159681767, 0.4092113241726023, 0.409008855172728, 0.4088300008067836, 0.40867181007764675, 0.4085317218599029, 0.408407511085529, 0.40829724264815237, 0.4081992318675821, 0.40811201054304014, 0.4080342977763359, 0.40796497487242434, 0.4079030637297341, 0.4078477082204888, 0.40779815813512854, 0.40775375532732694, 0.4077139217489719, 0.40767814910939887, 0.4076459899313777, 0.40761704980897107, 0.4075909807001969, 0.4075674751112004, 0.40754626104898006, 0.4075270976371014, 0.4075097713037278, 0.4074940924640431, 0.4074798926300871, 0.40746702189035566, 0.4074553467095884, 0.4074447480060124, 0.4074351194692461, 0.40742636608712335, 0.4074184028540577, 0.4074111536373006, 0.40740455018067134, 0.4073985312280841, 0.40739304175159274, 0.40738803227070597, 0.4073834582514948, 0.4073792795755322, 0.40737546007001413, 0.4073719670915441, 0.40736877115703424, 0.4073658456160254, 0.40736316635946135, 0.407360711560573, 0.4073584614440884, 0.4073563980804533, 0.4073545052021609, 0.40735276803964615, 0.40735117317451647, 0.40734970840815515, 0.4073483626439727]\n"
          ]
        }
      ],
      "source": [
        "# Train regularized logistic regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "\n",
        "r_lgr_weights = numpy.ones(x_train.shape[1])\n",
        "r_lgr_weights, r_lgr_objvals = gradient_descent(x_train, y_train, 0.5, 0.1, r_lgr_weights, max_epoch = 100)\n",
        "print(r_lgr_objvals)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHXe85GCMNB6"
      },
      "source": [
        "## 3.2. Stochastic gradient descent (SGD)\n",
        "\n",
        "Define new objective function $Q_i (w) = \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $. \n",
        "\n",
        "The stochastic gradient at $w$ is $g_i = \\frac{\\partial Q_i }{ \\partial w} = -\\frac{y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
        "\n",
        "You may need to implement a new function to calculate the new objective function and gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kuZ9ElooMNB6"
      },
      "outputs": [],
      "source": [
        "# Calculate the objective Q_i and the gradient of Q_i\n",
        "# Inputs:\n",
        "#     w: weights: d-by-1 matrix\n",
        "#     xi: data: 1-by-d matrix\n",
        "#     yi: label: scalar\n",
        "#     lam: scalar, the regularization parameter\n",
        "# Return:\n",
        "#     obj: scalar, the objective Q_i\n",
        "#     g: d-by-1 matrix, gradient of Q_i\n",
        "\n",
        "def stochastic_objective_gradient(w, xi, yi, lam):\n",
        "    assert(w.shape[0] == xi.shape[0])\n",
        "    z = numpy.matmul(numpy.multiply(yi, xi), w)\n",
        "    obj = numpy.log(1 + numpy.exp(-z)) + (lam/2) * numpy.linalg.norm(w)\n",
        "    g = -(numpy.multiply(yi, xi) / (1 + numpy.exp(z))) + lam * w\n",
        "\n",
        "    return obj, g\n",
        "\n",
        "# weights = numpy.ones(x_train.shape[1])\n",
        "# sum = 0\n",
        "# for i in range(x_train.shape[0]):\n",
        "#   sum += stochastic_objective_gradient(weights, x_train[i], y_train[i], 0)[0]\n",
        "\n",
        "# print(sum / x_train.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCbE2NcvMNB7"
      },
      "source": [
        "Hints:\n",
        "1. In every epoch, randomly permute the $n$ samples.\n",
        "2. Each epoch has $n$ iterations. In every iteration, use 1 sample, and compute the gradient and objective using the ``stochastic_objective_gradient`` function. In the next iteration, use the next sample, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VEjygXd1MNB7"
      },
      "outputs": [],
      "source": [
        "# SGD for solving logistic regression\n",
        "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
        "\n",
        "# Inputs:\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: scalar, the regularization parameter\n",
        "#     learning_rate: scalar\n",
        "#     w: weights: d-by-1 matrix, initialization of w\n",
        "#     max_epoch: integer, the maximal epochs\n",
        "# Return:\n",
        "#     \n",
        "#     w: weights: d-by-1 matrix, the solution\n",
        "#     objvals: a record of each epoch's objective value\n",
        "#     Record one objective value per epoch (not per iteration)\n",
        "\n",
        "\n",
        "def sgd(x, y, lam, learning_rate, w, max_epoch=100):\n",
        "    objvals = []\n",
        "    n = x.shape[0]\n",
        "\n",
        "    for epoch in range(max_epoch):\n",
        "      obj_sum = 0\n",
        "      sample_index = numpy.random.permutation(n)\n",
        "      for i in range(n):\n",
        "        iter_obj, gradient = stochastic_objective_gradient(w, x[sample_index[i]], y[sample_index[i]], lam)\n",
        "        w -= learning_rate * gradient\n",
        "        obj_sum += iter_obj\n",
        "      objvals.append(obj_sum /n)\n",
        "\n",
        "    return w, objvals\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xsmpSGbMNB7"
      },
      "source": [
        "Use sgd function to obtain your optimal weights and a list of objective values over each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7_DzPsJMNB7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4eb2183c-cc93-4c73-af57-22b8e13dd249"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12.477689422183477, 8.891199757764635, 5.5028829750330885, 2.6721606813633545, 1.1192485271796973, 0.5640681864950194, 0.3628142297933979, 0.2816767112684658, 0.2402820737641239, 0.21463133611495078, 0.19680282766433252, 0.1833589290100429, 0.17266831155651716, 0.16390481907841076, 0.1565009706050466, 0.15011517109302938, 0.14454694441381932, 0.13965113358443906, 0.13528370811689552, 0.1313737154442717, 0.12786239063009597, 0.12468864256973063, 0.12180218201740431, 0.11916676249450309, 0.11675675033385248, 0.11454000546941231, 0.1124951242633458, 0.11060224475222287, 0.10884947807400577, 0.10721354484234727, 0.10568609666307299, 0.10425717832629647, 0.10291878851811943, 0.1016601872170134, 0.10047449482211351, 0.09935576474380522, 0.09829569514534994, 0.0972953918044822, 0.09634270271463327, 0.09543552453168463, 0.0945724410259415, 0.09374899947322896, 0.09296198301386706, 0.09220764948800331, 0.09148582824742767, 0.09079386809780564, 0.09012689210774269, 0.0894879998477753, 0.0888718646478495, 0.08827674186476761, 0.08770450662327306, 0.08714917638817297, 0.08661502205466616, 0.08609590109707284, 0.08559382783779093, 0.08510805896697111, 0.08463596572862671, 0.08417809331105666, 0.08373280357760816, 0.08330049200858379, 0.08287912062563414, 0.08247078582174704, 0.08207184167674157, 0.08168419801982683, 0.0813062326577609, 0.08093662753074193, 0.08057715535046217, 0.0802265320620518, 0.07988276181894025, 0.07954752996240697, 0.07922086664689366, 0.07890036096610202, 0.0785866207188737, 0.07827895908415071, 0.07798014912561038, 0.07768621702516504, 0.07739769754072472, 0.07711537952568792, 0.0768392236255619, 0.07656851357476745, 0.07630185412702213, 0.07604101011718002, 0.07578550788272448, 0.07553348088620176, 0.07528741494276557, 0.075045154440774, 0.07480728600579518, 0.07457330573201722, 0.07434313662035107, 0.0741169608399188, 0.07389565554908954, 0.07367791768248658, 0.07346281977204648, 0.0732518604437675, 0.0730436917086196, 0.0728395374211481, 0.07263813208837284, 0.07243995124277748, 0.0722444984201531, 0.07205245337860205]\n"
          ]
        }
      ],
      "source": [
        "# Train logistic regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "\n",
        "slg_weights = numpy.ones(x_train.shape[1])\n",
        "slg_weights, slg_obj = sgd(x_train, y_train, 0, 0.001, slg_weights, max_epoch=100)\n",
        "print(slg_obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ioz-73EIMNB7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc46e00a-30c2-47af-9280-73a7fe7fa1b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12.250871665667013, 6.385229851636004, 2.311550447834774, 0.7778719401852072, 0.5152991033291127, 0.4541049014617086, 0.4307558714281222, 0.4201720243559016, 0.41473296998867093, 0.41178076556248877, 0.4101827946602554, 0.4090643249390571, 0.40873172191587953, 0.40847951643027214, 0.4082596540736584, 0.4080747871659077, 0.4079367945972353, 0.40804631130354846, 0.4079064485728821, 0.40795851950064804, 0.40793944584711983, 0.4077730243235814, 0.40798099979120894, 0.4078245746912622, 0.4078895382735313, 0.4078003778357568, 0.4077534690325691, 0.407861667144938, 0.40779474748668537, 0.4079658140009135, 0.40787436672336413, 0.40796420720576376, 0.4078627052593073, 0.40775774698053446, 0.4078451857867479, 0.4078841858209399, 0.4078520526344476, 0.4078874991176101, 0.4078790932153902, 0.4077705650852803, 0.4078894410009772, 0.40782543428143453, 0.40780999158873155, 0.4079243057367667, 0.40780411240190134, 0.40780361414685196, 0.4078094566610879, 0.40792094213948, 0.4078713067178158, 0.4078863975330319, 0.4078807410197984, 0.40790066289435134, 0.4077663288293227, 0.407841154552745, 0.40791376782426986, 0.40777441061104774, 0.4077656984446419, 0.40793603965088837, 0.4078694558264689, 0.4079554642028117, 0.4079189977622732, 0.40775601288906443, 0.4077489366070426, 0.4079650844478902, 0.4077819700646405, 0.40785551776014134, 0.40786475863701854, 0.40783511671310296, 0.40788433211872954, 0.4077687089897198, 0.4078786814578639, 0.40783877450318196, 0.4078406283315333, 0.40782969974269545, 0.40790110786694256, 0.4077821955409875, 0.4079439271158958, 0.40774684867680294, 0.40787556455879637, 0.4078873577621621, 0.4078789946591522, 0.4079208820641661, 0.407841902395206, 0.40780582652208275, 0.40777370846182065, 0.40794837067705997, 0.4079444310714386, 0.4076848290781099, 0.4079124387504731, 0.4079395395242335, 0.40787543335023124, 0.40775931133601084, 0.4078931865479886, 0.40791559272683925, 0.4078921016951469, 0.4077888513741375, 0.40786570444193165, 0.40777192627166364, 0.4079180355522985, 0.40785379867751903]\n"
          ]
        }
      ],
      "source": [
        "# Train regularized logistic regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "\n",
        "slgr_weights = numpy.ones(x_train.shape[1])\n",
        "slgr_weights, slgr_obj = sgd(x_train, y_train, 0.5, 0.001, slgr_weights, max_epoch=100)\n",
        "print(slgr_obj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Hej7BDvMNB7"
      },
      "source": [
        "## 3.3 Mini-Batch Gradient Descent (MBGD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ha59VNHDMNB7"
      },
      "source": [
        "Define $Q_I (w) = \\frac{1}{b} \\sum_{i \\in I} \\log \\Big( 1 + \\exp \\big( - y_i x_i^T w \\big) \\Big) + \\frac{\\lambda}{2} \\| w \\|_2^2 $, where $I$ is a set containing $b$ indices randomly drawn from $\\{ 1, \\cdots , n \\}$ without replacement.\n",
        "\n",
        "The stochastic gradient at $w$ is $g_I = \\frac{\\partial Q_I }{ \\partial w} = \\frac{1}{b} \\sum_{i \\in I} \\frac{- y_i x_i }{1 + \\exp ( y_i x_i^T w)} + \\lambda w$.\n",
        "\n",
        "You may need to implement a new function to calculate the new objective function and gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GBkd-QLVMNB8"
      },
      "outputs": [],
      "source": [
        "# Calculate the objective Q_I and the gradient of Q_I\n",
        "# Inputs:\n",
        "#     w: weights: d-by-1 matrix\n",
        "#     xi: data: b-by-d matrix\n",
        "#     yi: label: scalar\n",
        "#     lam: scalar, the regularization parameter\n",
        "# Return:\n",
        "#     obj: scalar, the objective Q_i\n",
        "#     g: d-by-1 matrix, gradient of Q_i\n",
        "\n",
        "def mb_objective_gradient(w, xi, yi, lam):\n",
        "    assert(w.shape[0] == xi.shape[1])\n",
        "    \n",
        "    obj_sum = 0\n",
        "    grad_sum = 0\n",
        "    b = xi.shape[0]\n",
        "\n",
        "    for i in range(b):\n",
        "      z = numpy.matmul(numpy.multiply(yi[i], xi[i]), w)\n",
        "      obj_sum += numpy.log(1 + numpy.exp(-z)) \n",
        "      grad_sum = grad_sum - numpy.multiply(yi[i], xi[i]) / (1 + numpy.exp(z))\n",
        "    \n",
        "    return (obj_sum + (lam / 2) * numpy.linalg.norm(w))/b, (grad_sum + lam * w)/b "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiMRKJmKMNB8"
      },
      "source": [
        "Hints:\n",
        "1. In every epoch, randomly permute the $n$ samples (just like SGD).\n",
        "2. Each epoch has $\\frac{n}{b}$ iterations. In every iteration, use $b$ samples, and compute the gradient and objective using the ``mb_objective_gradient`` function. In the next iteration, use the next $b$ samples, and so on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KyfZkUGSMNB8"
      },
      "outputs": [],
      "source": [
        "# MBGD for solving logistic regression\n",
        "# You will need to do iterative process (loops) to obtain optimal weights in this function\n",
        "\n",
        "# Inputs:\n",
        "#     x: data: n-by-d matrix\n",
        "#     y: label: n-by-1 matrix\n",
        "#     lam: scalar, the regularization parameter\n",
        "#     learning_rate: scalar\n",
        "#     w: weights: d-by-1 matrix, initialization of w\n",
        "#     max_epoch: integer, the maximal epochs\n",
        "# Return:\n",
        "#     w: weights: d-by-1 matrix, the solution\n",
        "#     objvals: a record of each epoch's objective value\n",
        "#     Record one objective value per epoch (not per iteration)\n",
        "\n",
        "def mbgd(x, y, lam, learning_rate, w, max_epoch=100):\n",
        "    b = 20\n",
        "    obj_vals = []\n",
        "    obj_sum = 0\n",
        "\n",
        "    for epoch in range(max_epoch):\n",
        "      obj_sum = 0\n",
        "\n",
        "      for i in range(x.shape[0] // b):\n",
        "        selected_i = numpy.random.choice(x.shape[0], b)\n",
        "        xi, yi = x[selected_i, :], y[selected_i, :]\n",
        "        obj, grad = mb_objective_gradient(w, xi, yi, lam)\n",
        "        \n",
        "        w -= learning_rate * grad\n",
        "        obj_sum += obj\n",
        "\n",
        "      obj_sum = obj_sum / (x.shape[0] // b)\n",
        "      \n",
        "      obj_vals.append(obj_sum)\n",
        "\n",
        "\n",
        "    return w, obj_vals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYW-_W-EMNB8"
      },
      "source": [
        "Use mbgd function to obtain your optimal weights and a list of objective values over each epoch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qBTFkKuxMNB8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e95e5669-d57b-455b-de67-0c65b02d4a67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12.77229538345532, 9.186891571270134, 5.795330372578098, 2.9309490408750416, 1.3054625338839276, 0.5599927367388827, 0.3422148755728321, 0.2935039460947751, 0.22963976154369775, 0.22165295747430044, 0.2000243194145498, 0.16556884370549727, 0.1400335765315187, 0.16241226942824633, 0.16124571944548655, 0.1454401054205138, 0.13929877906564578, 0.12220091101815736, 0.11770157308916838, 0.14608189861437937, 0.1277846608928734, 0.13096719584959354, 0.12242367982378834, 0.14657621034587495, 0.13702692405753217, 0.10652635931352043, 0.10849953206878876, 0.09741219859488524, 0.12639403228437696, 0.12263136492077593, 0.1293918385860967, 0.09760067533395357, 0.09866379416831958, 0.09040434936753641, 0.10862316516626955, 0.09237059554642615, 0.09568949647590363, 0.10758521200462559, 0.10243944666183914, 0.10076669301386972, 0.08813505446826009, 0.08658803131025586, 0.09563927682720844, 0.08363897438862233, 0.081683599739371, 0.10162147296144922, 0.09682117164576369, 0.09672353107729147, 0.11630273762448433, 0.0735756510724447, 0.09337111439535381, 0.08232580861021312, 0.10061807859539976, 0.09953506411530468, 0.07252404766043223, 0.07831857001919479, 0.07069037215533615, 0.08322858872931273, 0.09074553479838954, 0.07757948644194114, 0.060367717126394396, 0.10165561542517665, 0.08436651960087574, 0.07163145649918051, 0.0944719282064137, 0.09103161573959771, 0.09303670549549743, 0.08037097954213855, 0.0905264359874939, 0.08018969865592801, 0.07333448421607808, 0.07510869034875275, 0.08633378841228419, 0.06294310427728532, 0.08419337515078842, 0.06600205849594597, 0.09952462977503274, 0.06748618651657369, 0.09042715093608548, 0.07677994179956826, 0.09779569762025088, 0.07784314032202355, 0.08341241803964677, 0.07025518188790696, 0.06278321647931898, 0.0885204510889091, 0.06489539527345255, 0.0647609595089847, 0.06878589737273118, 0.07202506982306713, 0.07036867721735311, 0.08464931420927489, 0.0702582635247724, 0.09375495548138134, 0.07923002252209797, 0.07277606875378402, 0.07423048392036806, 0.08093883005091738, 0.08837279802337598, 0.08419970576001946]\n"
          ]
        }
      ],
      "source": [
        "# Train logistic regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "\n",
        "mb_weights = numpy.ones(x_train.shape[1])\n",
        "mb_weights, mb_obj = mbgd(x_train, y_train, 0, 0.02, mb_weights, max_epoch = 100)\n",
        "print(mb_obj)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ck1Exor4MNB8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dd48e53-de45-48c2-bb60-f4f47164d6b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[12.647369879471631, 8.293278299171478, 5.6524028513229805, 3.0131822329305398, 1.3351646536973574, 0.6312749310140889, 0.400774842873256, 0.26630617660938494, 0.2594472799518497, 0.2088918416187913, 0.2419521951314044, 0.16977367582640532, 0.2025656993567212, 0.20119900280340025, 0.1889547794728166, 0.16003853737954724, 0.16176140976646122, 0.16662718943895952, 0.15900957886679326, 0.1425502227229947, 0.16922537760461676, 0.16877135986549183, 0.1567170171395924, 0.14603539674307864, 0.13882206161122215, 0.1336220601984042, 0.14986026307966016, 0.1512590198267697, 0.12178656338017374, 0.1419626476625713, 0.13553575616311375, 0.14498115618558366, 0.14079226999662928, 0.14646043508767395, 0.13847299658032605, 0.11229708540294346, 0.12851308215694898, 0.12766183833420294, 0.13462219803787215, 0.12848118984011728, 0.11714647292344095, 0.11825497548377652, 0.139465035242461, 0.11299071739780787, 0.10727083980975792, 0.14030005793322115, 0.12362761706144682, 0.11493580531705766, 0.12168569909049194, 0.12957610267774833, 0.112644371228403, 0.12857608303200832, 0.11779958096165653, 0.13014351225727372, 0.13069834523352533, 0.11359971433988393, 0.14776739368520586, 0.11338687709862111, 0.11323146487286978, 0.12630666334143423, 0.12349248255644919, 0.11610987558475597, 0.1282080558879525, 0.14379968686146088, 0.13240639584303487, 0.11805895343121225, 0.12115341111707272, 0.11127150702319705, 0.1320004792176993, 0.1172897713932009, 0.12246166876751875, 0.12039160496447793, 0.11921112387799365, 0.11632548816539591, 0.13657549367918295, 0.1496159062630229, 0.12857515922180285, 0.12736520927314593, 0.10884806371171073, 0.10329729836602747, 0.10423698484926484, 0.13195200178772457, 0.1337152074492252, 0.1253798379971686, 0.1348502199351253, 0.11842853033227088, 0.13230574901360512, 0.13013242152049306, 0.11326874731078859, 0.11693594523328915, 0.11552750193599502, 0.11500506274022712, 0.12446278014984373, 0.15405028582735888, 0.11770280500565562, 0.11404101395127282, 0.1036164597220778, 0.11550419776097237, 0.13488332668542524, 0.125694198373774]\n"
          ]
        }
      ],
      "source": [
        "# Train regularized logistic regression\n",
        "# You should get the optimal weights and a list of objective values by using gradient_descent function.\n",
        "\n",
        "rmb_weights = numpy.ones(x_train.shape[1])\n",
        "rmb_weights, rmb_obj = mbgd(x_train, y_train, 0.5, 0.02, rmb_weights, max_epoch = 100)\n",
        "print(rmb_obj)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBHfisWTMNB8"
      },
      "source": [
        "# 4. Compare GD, SGD, MBGD\n",
        "\n",
        "### Plot objective function values against epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtU20I5uMNB9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 851
        },
        "outputId": "25b42334-9d34-49e7-b756-173cec3d3aa4"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "lgr_objvals, r_lgr_objvals, slg_obj, slgr_obj, mb_obj, rmb_obj;\n",
        "epochs = numpy.arange(100)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(epochs, lgr_objvals, color = \"red\")\n",
        "ax.plot(epochs, r_lgr_objvals, color = \"blue\")\n",
        "ax.set(xlabel = \"epoch #\", ylabel = \"loss\", title = \"Loss for Gradient(R) vs Regularized Gradient(B)\")\n",
        "ax.grid()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(epochs, slg_obj, color = \"red\")\n",
        "ax.plot(epochs, slgr_obj, color = \"blue\")\n",
        "ax.set(xlabel = \"epoch #\", ylabel = \"loss\", title = \"Loss for Gradient(R) vs Regularized Gradient(B)\")\n",
        "ax.grid()\n",
        "\n",
        "plt.show()\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(epochs, mb_obj, color = \"red\")\n",
        "ax.plot(epochs, rmb_obj, color = \"blue\")\n",
        "ax.set(xlabel = \"epoch #\", ylabel = \"loss\", title = \"Loss for Gradient(R) vs Regularized Gradient(B)\")\n",
        "ax.grid()\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBLGffkoMNB9"
      },
      "source": [
        "# 5. Prediction\n",
        "### Compare the training and testing accuracy for logistic regression and regularized logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_7BmEN4kMNB9"
      },
      "outputs": [],
      "source": [
        "# Predict class label\n",
        "# Inputs:\n",
        "#     w: weights: d-by-1 matrix\n",
        "#     X: data: m-by-d matrix\n",
        "# Return:\n",
        "#     f: m-by-1 matrix, the predictions\n",
        "from decimal import Decimal\n",
        "\n",
        "def predict(w, X):\n",
        "    y_pred = numpy.matmul(X, w)\n",
        "    fix_pred = lambda x: 1 if x > 0 else -1\n",
        "    y_pred = [fix_pred(x) for x in y_pred]\n",
        "    \n",
        "    return y_pred\n",
        "\n",
        "def calc_accuracy(y_pred, y_train):\n",
        "    y_train = numpy.squeeze(y_train)\n",
        "    return Decimal(1 - numpy.count_nonzero(y_train - y_pred) / len(y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4BBqx8dMNB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7defb284-1424-4e69-916c-5055b485123f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9516483516483515980866059180698357522487640380859375\n",
            "0.95824175824175827909101599288987927138805389404296875\n",
            "0.98681318681318686003578477539122104644775390625\n",
            "0.95824175824175827909101599288987927138805389404296875\n",
            "0.98021978021978017903137470057117752730846405029296875\n",
            "0.984615384615384670041748904623091220855712890625\n"
          ]
        }
      ],
      "source": [
        "# evaluate training error of logistic regression and regularized version\n",
        "\n",
        "print(calc_accuracy(predict(lgr_weights, x_train), y_train))\n",
        "print(calc_accuracy(predict(r_lgr_weights, x_train), y_train))\n",
        "\n",
        "print(calc_accuracy(predict(slg_weights, x_train), y_train))\n",
        "print(calc_accuracy(predict(slgr_weights, x_train), y_train))\n",
        "\n",
        "print(calc_accuracy(predict(mb_weights, x_train), y_train))\n",
        "print(calc_accuracy(predict(rmb_weights, x_train), y_train))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxxngUmnMNB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53251a56-3ca4-4c20-8347-7d00b67c787d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.97368421052631581869007959539885632693767547607421875\n",
            "0.9912280701754385692225923776277340948581695556640625\n",
            "0.982456140350877138445184755255468189716339111328125\n",
            "0.9912280701754385692225923776277340948581695556640625\n",
            "0.982456140350877138445184755255468189716339111328125\n",
            "0.9912280701754385692225923776277340948581695556640625\n"
          ]
        }
      ],
      "source": [
        "# evaluate testing error of logistic regression and regularized version\n",
        "\n",
        "print(calc_accuracy(predict(lgr_weights, x_test), y_test))\n",
        "print(calc_accuracy(predict(r_lgr_weights, x_test), y_test))\n",
        "\n",
        "print(calc_accuracy(predict(slg_weights, x_test), y_test))\n",
        "print(calc_accuracy(predict(slgr_weights, x_test), y_test))\n",
        "\n",
        "print(calc_accuracy(predict(mb_weights, x_test), y_test))\n",
        "print(calc_accuracy(predict(rmb_weights, x_test), y_test))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQwqP1ERMNB9"
      },
      "source": [
        "# 6. Parameters tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VocKBoKjMNB9"
      },
      "source": [
        "### In this section, you may try different combinations of parameters (regularization value, learning rate, etc) to see their effects on the model. (Open ended question)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb5HeYJTMNB-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "228fbb3f-a88c-408d-8871-d84e065a3c9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[14.297006648784398, 13.487196995212255, 12.680314417382805, 11.877529688682587, 11.08026771067247, 10.289585321228493, 9.505916109609336, 8.729583545229339, 7.961259791765186, 7.202209889972502, 6.454555042442677, 5.721607110714859, 5.008154179556991, 4.320756505300139, 3.6681026986839815, 3.061513353415488, 2.5158657802491504, 2.0466014010750095, 1.661304152690012, 1.3563052851493502, 1.1207198911075398, 0.9390293448585079, 0.7973773189093984, 0.6853054391565628, 0.5960466340991138, 0.5249338689093612, 0.46834866826636956, 0.42337017982468056, 0.3875755049687151, 0.358887651803037, 0.3355830477258795, 0.31633416975228706, 0.3001751192454097, 0.2864144202391631, 0.2745502582929078, 0.26420953366798616, 0.25510842360303604, 0.24702713003508658, 0.2397931533599093, 0.23326969330876146, 0.2273472841981949, 0.22193760035976381, 0.21696879473671352, 0.21238195725427442, 0.20812840604292784, 0.2041676038977612, 0.20046554657772966, 0.19699350859680445, 0.1937270609582381, 0.1906452966986202]\n",
            "[19.77423222383606, 14.76390094738342, 10.77385340172258, 7.611702329209891, 5.133513664839476, 3.250152548517601, 1.9365995213202707, 1.1943914573931067, 0.891433836948248, 0.7836971804620203, 0.7415581626437086, 0.7228854122675733, 0.7138150121311707, 0.7091254608219841, 0.706598470495831, 0.7052006632553054, 0.70441611080409, 0.7039736469307105, 0.7037253068330237, 0.7035881599826685, 0.7035148552677732, 0.7034780287915501, 0.703461763375563, 0.7034567876704116, 0.7034577241241455, 0.7034614881756978, 0.7034663470531264, 0.7034713622595212, 0.7034760578804862, 0.7034802229968544, 0.7034837942579115, 0.7034867865695082, 0.7034892527087765, 0.7034912603057846, 0.7034928791991384, 0.7034941749263437, 0.7034952057803758, 0.7034960218816293, 0.7034966653361131, 0.7034971709305606, 0.7034975670467154, 0.703497876617353, 0.7034981180308331, 0.7034983059406106, 0.7034984519646665, 0.7034985652754631, 0.7034986530890106, 0.703498721065035, 0.7034987736311373, 0.7034988142432816]\n",
            "[12.477406061703403, 8.88824650815189, 5.495578527162337, 2.6671740105500588, 1.1240268177547008, 0.568326968287144, 0.3661440129671688, 0.28406160517239826, 0.24179562506579758, 0.21578412613876716, 0.19767109062502305, 0.1840709040072375, 0.17326461876286472, 0.1643718353217966, 0.15688722301886746, 0.15043596323040626, 0.14482389886190405, 0.13988771486005905, 0.13551298917096768, 0.13159895121094986, 0.12807191982532296, 0.12487963707406206, 0.12198093495160883, 0.11933442034757422, 0.11691096886880281, 0.11468441640340131, 0.11263513665605238, 0.11073709317939227, 0.10897538737831954, 0.1073391238839811, 0.10581060569268552, 0.10437950970029233, 0.10303432994920378, 0.10177183409542204, 0.10058621791788845, 0.09946384064574224, 0.09840203706237421, 0.09739628173613275, 0.09644027101164689, 0.09553074080803996, 0.09466612347450012, 0.09384007016118856, 0.09305015943951232, 0.092294420115769, 0.09156846843045098, 0.09087682520581093, 0.09020961166980647, 0.08956719345446652, 0.08894738778124427, 0.08835110854661737]\n",
            "[11.319048106214128, 2.25835755415989, 0.7609452013577843, 0.7065189421682725, 0.7041013500956337, 0.7044574706023553, 0.7051475483285594, 0.7039214105935299, 0.7042264732967678, 0.7048559188111013, 0.7044048856223788, 0.7053171760970078, 0.704290877365958, 0.70342181987028, 0.7048797518939379, 0.7051080992490752, 0.7043862012970635, 0.7044535595009259, 0.7041986915083414, 0.7056719760149189, 0.7040218488907803, 0.7038637546398812, 0.7043159889345204, 0.705051185130165, 0.7052529643856027, 0.7035250320484925, 0.7047138742112881, 0.704912479701634, 0.7043451383353815, 0.7047155824710861, 0.7040929004046044, 0.7050005161135011, 0.7044305476803172, 0.7043399024032146, 0.7042203288102029, 0.7047635710221051, 0.7050576483388312, 0.7039739688124599, 0.7049728235879154, 0.7044782960025556, 0.7047969215230511, 0.7036736576813309, 0.7045633507365885, 0.7048037113766329, 0.7044360268516523, 0.7043114656165447, 0.7045669214000488, 0.7048961069593797, 0.704804655745422, 0.704480921210089]\n",
            "[13.179677755550257, 8.589390547875094, 5.853161258160356, 3.0213475572061426, 1.2818898040973146, 0.5761645192254238, 0.37800981472885786, 0.26419044915559003, 0.2654471043425082, 0.25554291498069504, 0.2159265896827183, 0.19418174232396934, 0.17320114407306553, 0.17815286828005256, 0.18842627762214006, 0.1526463368914258, 0.12760127724751616, 0.14813711602729643, 0.1408741955585985, 0.16448104663970498, 0.13389900252652587, 0.11373420784806397, 0.12114910589214074, 0.12760111058807777, 0.12333298825188174, 0.10923142979866539, 0.13462774608341446, 0.1056792136068304, 0.09307981304369535, 0.09065524104314025, 0.10767222936874897, 0.11712369728710867, 0.12997375709493758, 0.09653065535451483, 0.10478941675382694, 0.10032098320739413, 0.1018814579897719, 0.0913828237039012, 0.09613338701698906, 0.09726612851595774, 0.09224816714918022, 0.09288837955178889, 0.08766639245766898, 0.08565341222404042, 0.08159968732695994, 0.09983724809801515, 0.0769337969450224, 0.08859075518193761, 0.09131000584340572, 0.08354022900576309]\n",
            "[11.885705196942787, 8.658202543806285, 5.224611217872139, 2.2622825845422203, 0.9367054258984683, 0.5050816713651027, 0.36888895879566513, 0.3211831700987446, 0.27660566586680574, 0.26151318802950857, 0.28123332387018113, 0.24833891332424776, 0.2696766429772679, 0.23191190362926964, 0.23307386173848785, 0.20596062124403772, 0.22498654144661445, 0.21359937254553565, 0.21179298297848048, 0.2554858260066927, 0.20269494964425877, 0.2120676302619291, 0.2019906427839632, 0.20409932170404252, 0.22459902545148705, 0.2247869866936817, 0.21333032164520385, 0.20181609543890805, 0.1943581459156022, 0.1867568288853639, 0.22327674281153478, 0.20882122993488297, 0.20482839894853672, 0.19554699772931441, 0.1980202071861149, 0.2114451836863882, 0.21107170036350478, 0.20904026311002236, 0.1932465846754309, 0.19799433172816033, 0.2037888696537831, 0.22387902571028998, 0.1958330640414685, 0.20227891743564144, 0.22336675755242635, 0.20653777973340073, 0.2030150643854944, 0.21402846769775657, 0.19187030334331637, 0.2135013669019048]\n"
          ]
        }
      ],
      "source": [
        "# Retrain each model on lower epoch size & larger regularization term\n",
        "\n",
        "# Normal Gradient Descent\n",
        "lgr_weights = numpy.ones(x_train.shape[1])\n",
        "lgr_weights, lgr_objvals = gradient_descent(x_train, y_train, 0, 0.1, lgr_weights, max_epoch = 50)\n",
        "print(lgr_objvals)\n",
        "\n",
        "# Regularized Gradient Descent\n",
        "r_lgr_weights = numpy.ones(x_train.shape[1])\n",
        "r_lgr_weights, r_lgr_objvals = gradient_descent(x_train, y_train, 1, 0.1, r_lgr_weights, max_epoch = 50)\n",
        "print(r_lgr_objvals)\n",
        "\n",
        "# Stochastic Gradient Descent\n",
        "slg_weights = numpy.ones(x_train.shape[1])\n",
        "slg_weights, slg_obj = sgd(x_train, y_train, 0, 0.001, slg_weights, max_epoch=50)\n",
        "print(slg_obj)\n",
        "\n",
        "# Regularized Stochastic Gradient Descent\n",
        "slgr_weights = numpy.ones(x_train.shape[1])\n",
        "slgr_weights, slgr_obj = sgd(x_train, y_train, 1, 0.001, slgr_weights, max_epoch=50)\n",
        "print(slgr_obj)\n",
        "\n",
        "# Mini-batch Gradient Descent\n",
        "mb_weights = numpy.ones(x_train.shape[1])\n",
        "mb_weights, mb_obj = mbgd(x_train, y_train, 0, 0.02, mb_weights, max_epoch = 50)\n",
        "print(mb_obj)\n",
        "\n",
        "# Regularized Mini-batch Gradient Descent\n",
        "rmb_weights = numpy.ones(x_train.shape[1])\n",
        "rmb_weights, rmb_obj = mbgd(x_train, y_train, 1, 0.02, rmb_weights, max_epoch = 50)\n",
        "print(rmb_obj)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding notes here as I play around with the model above\n",
        "# Note 1: Regularized term = 5 -> training data loss increases significantly, same with accuracy\n",
        "# Note 2: Regularized value: 0.5 ~ 1 seems to be consistent\n",
        "# Note 3: Epochs = 50 significantly improves training accuracy, I find this strange, shouldn't training increase regardless of epoch number\n",
        "#         and testing accuracy should decrease.\n",
        "# Result: After checking testing accuracy, ~ 1-2% decrease for all models\n",
        "#         Not that that accounts for much given how small the data set is & high the accuracy is. There's probably only 1 or 2 datapoints different.\n",
        "\n",
        "print(calc_accuracy(predict(lgr_weights, x_train), y_train))\n",
        "print(calc_accuracy(predict(r_lgr_weights, x_train), y_train))\n",
        "\n",
        "print(calc_accuracy(predict(slg_weights, x_train), y_train))\n",
        "print(calc_accuracy(predict(slgr_weights, x_train), y_train))\n",
        "\n",
        "print(calc_accuracy(predict(mb_weights, x_train), y_train))\n",
        "print(calc_accuracy(predict(rmb_weights, x_train), y_train))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKtu2qy3bw-8",
        "outputId": "4bb356e2-f77c-4879-a0de-9a4875796206"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.91428571428571425716569365249597467482089996337890625\n",
            "0.9494505494505494080925700473017059266567230224609375\n",
            "0.97362637362637360904926708826678805053234100341796875\n",
            "0.9472527472527472180985341765335761010646820068359375\n",
            "0.97582417582417579904330295903491787612438201904296875\n",
            "0.97362637362637360904926708826678805053234100341796875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate testing error of logistic regression and regularized version\n",
        "\n",
        "print(calc_accuracy(predict(lgr_weights, x_test), y_test))\n",
        "print(calc_accuracy(predict(r_lgr_weights, x_test), y_test))\n",
        "\n",
        "print(calc_accuracy(predict(slg_weights, x_test), y_test))\n",
        "print(calc_accuracy(predict(slgr_weights, x_test), y_test))\n",
        "\n",
        "print(calc_accuracy(predict(mb_weights, x_test), y_test))\n",
        "print(calc_accuracy(predict(rmb_weights, x_test), y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P-I72ZsAeqT3",
        "outputId": "03481d02-eaf0-45de-f3a7-3a1c380bbf78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9298245614035087758253439460531808435916900634765625\n",
            "0.97368421052631581869007959539885632693767547607421875\n",
            "0.982456140350877138445184755255468189716339111328125\n",
            "0.982456140350877138445184755255468189716339111328125\n",
            "0.982456140350877138445184755255468189716339111328125\n",
            "0.9912280701754385692225923776277340948581695556640625\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "8a94588eda9d64d9e9a351ab8144e55b1fabf5113b54e67dd26a8c27df0381b3"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}